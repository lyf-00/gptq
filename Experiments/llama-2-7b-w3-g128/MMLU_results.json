{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-anatomy": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.04188307537595853,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.04188307537595853
    },
    "hendrycksTest-astronomy": {
      "acc": 0.40789473684210525,
      "acc_stderr": 0.03999309712777471,
      "acc_norm": 0.40789473684210525,
      "acc_norm_stderr": 0.03999309712777471
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.47547169811320755,
      "acc_stderr": 0.030735822206205608,
      "acc_norm": 0.47547169811320755,
      "acc_norm_stderr": 0.030735822206205608
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04155319955593146,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04155319955593146
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4046242774566474,
      "acc_stderr": 0.03742461193887248,
      "acc_norm": 0.4046242774566474,
      "acc_norm_stderr": 0.03742461193887248
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.04280105837364395,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "hendrycksTest-computer_security": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.40425531914893614,
      "acc_stderr": 0.03208115750788684,
      "acc_norm": 0.40425531914893614,
      "acc_norm_stderr": 0.03208115750788684
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2894736842105263,
      "acc_stderr": 0.04266339443159392,
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.04266339443159392
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878151,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.22486772486772486,
      "acc_stderr": 0.02150209607822914,
      "acc_norm": 0.22486772486772486,
      "acc_norm_stderr": 0.02150209607822914
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3492063492063492,
      "acc_stderr": 0.04263906892795133,
      "acc_norm": 0.3492063492063492,
      "acc_norm_stderr": 0.04263906892795133
    },
    "hendrycksTest-global_facts": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.4645161290322581,
      "acc_stderr": 0.028372287797962956,
      "acc_norm": 0.4645161290322581,
      "acc_norm_stderr": 0.028372287797962956
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3251231527093596,
      "acc_stderr": 0.032957975663112704,
      "acc_norm": 0.3251231527093596,
      "acc_norm_stderr": 0.032957975663112704
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5212121212121212,
      "acc_stderr": 0.03900828913737302,
      "acc_norm": 0.5212121212121212,
      "acc_norm_stderr": 0.03900828913737302
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.5595854922279793,
      "acc_stderr": 0.03582724530036095,
      "acc_norm": 0.5595854922279793,
      "acc_norm_stderr": 0.03582724530036095
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.38461538461538464,
      "acc_stderr": 0.02466674491518722,
      "acc_norm": 0.38461538461538464,
      "acc_norm_stderr": 0.02466674491518722
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3907563025210084,
      "acc_stderr": 0.031693802357129965,
      "acc_norm": 0.3907563025210084,
      "acc_norm_stderr": 0.031693802357129965
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33774834437086093,
      "acc_stderr": 0.03861557546255169,
      "acc_norm": 0.33774834437086093,
      "acc_norm_stderr": 0.03861557546255169
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5559633027522936,
      "acc_stderr": 0.021302621211654518,
      "acc_norm": 0.5559633027522936,
      "acc_norm_stderr": 0.021302621211654518
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.42592592592592593,
      "acc_stderr": 0.03372343271653063,
      "acc_norm": 0.42592592592592593,
      "acc_norm_stderr": 0.03372343271653063
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4068627450980392,
      "acc_stderr": 0.03447891136353382,
      "acc_norm": 0.4068627450980392,
      "acc_norm_stderr": 0.03447891136353382
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3881856540084388,
      "acc_stderr": 0.03172295004332331,
      "acc_norm": 0.3881856540084388,
      "acc_norm_stderr": 0.03172295004332331
    },
    "hendrycksTest-human_aging": {
      "acc": 0.47085201793721976,
      "acc_stderr": 0.03350073248773404,
      "acc_norm": 0.47085201793721976,
      "acc_norm_stderr": 0.03350073248773404
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.46564885496183206,
      "acc_stderr": 0.043749285605997376,
      "acc_norm": 0.46564885496183206,
      "acc_norm_stderr": 0.043749285605997376
    },
    "hendrycksTest-international_law": {
      "acc": 0.6115702479338843,
      "acc_stderr": 0.04449270350068382,
      "acc_norm": 0.6115702479338843,
      "acc_norm_stderr": 0.04449270350068382
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4351851851851852,
      "acc_stderr": 0.04792898170907062,
      "acc_norm": 0.4351851851851852,
      "acc_norm_stderr": 0.04792898170907062
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4110429447852761,
      "acc_stderr": 0.038656978537853624,
      "acc_norm": 0.4110429447852761,
      "acc_norm_stderr": 0.038656978537853624
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.32142857142857145,
      "acc_stderr": 0.04432804055291519,
      "acc_norm": 0.32142857142857145,
      "acc_norm_stderr": 0.04432804055291519
    },
    "hendrycksTest-management": {
      "acc": 0.47572815533980584,
      "acc_stderr": 0.049449010929737795,
      "acc_norm": 0.47572815533980584,
      "acc_norm_stderr": 0.049449010929737795
    },
    "hendrycksTest-marketing": {
      "acc": 0.6025641025641025,
      "acc_stderr": 0.03205953453789293,
      "acc_norm": 0.6025641025641025,
      "acc_norm_stderr": 0.03205953453789293
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.578544061302682,
      "acc_stderr": 0.017657976412654857,
      "acc_norm": 0.578544061302682,
      "acc_norm_stderr": 0.017657976412654857
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.026788811931562767,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.026788811931562767
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.264804469273743,
      "acc_stderr": 0.014756906483260659,
      "acc_norm": 0.264804469273743,
      "acc_norm_stderr": 0.014756906483260659
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4411764705882353,
      "acc_stderr": 0.028431095444176643,
      "acc_norm": 0.4411764705882353,
      "acc_norm_stderr": 0.028431095444176643
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5209003215434084,
      "acc_stderr": 0.028373270961069414,
      "acc_norm": 0.5209003215434084,
      "acc_norm_stderr": 0.028373270961069414
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.02764847787741332,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.02764847787741332
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3262411347517731,
      "acc_stderr": 0.027968453043563168,
      "acc_norm": 0.3262411347517731,
      "acc_norm_stderr": 0.027968453043563168
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3070404172099087,
      "acc_stderr": 0.011780959114513762,
      "acc_norm": 0.3070404172099087,
      "acc_norm_stderr": 0.011780959114513762
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.48161764705882354,
      "acc_stderr": 0.030352303395351964,
      "acc_norm": 0.48161764705882354,
      "acc_norm_stderr": 0.030352303395351964
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.35784313725490197,
      "acc_stderr": 0.019393058402355442,
      "acc_norm": 0.35784313725490197,
      "acc_norm_stderr": 0.019393058402355442
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5,
      "acc_stderr": 0.04789131426105757,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.04789131426105757
    },
    "hendrycksTest-security_studies": {
      "acc": 0.46938775510204084,
      "acc_stderr": 0.031949171367580624,
      "acc_norm": 0.46938775510204084,
      "acc_norm_stderr": 0.031949171367580624
    },
    "hendrycksTest-sociology": {
      "acc": 0.6069651741293532,
      "acc_stderr": 0.0345368246603156,
      "acc_norm": 0.6069651741293532,
      "acc_norm_stderr": 0.0345368246603156
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-virology": {
      "acc": 0.35542168674698793,
      "acc_stderr": 0.03726214354322416,
      "acc_norm": 0.35542168674698793,
      "acc_norm_stderr": 0.03726214354322416
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5906432748538012,
      "acc_stderr": 0.03771283107626545,
      "acc_norm": 0.5906432748538012,
      "acc_norm_stderr": 0.03771283107626545
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "/scratch/yifei/llama2-hf/llama-2-7b",
    "model_args": null,
    "num_fewshot": 5,
    "batch_size": 16,
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  },
  "average_acc": 0.4172092311117495
}