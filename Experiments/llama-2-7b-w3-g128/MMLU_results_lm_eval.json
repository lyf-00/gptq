{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768081,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768081
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04292596718256981,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04292596718256981
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3618421052631579,
      "acc_stderr": 0.03910525752849724,
      "acc_norm": 0.3618421052631579,
      "acc_norm_stderr": 0.03910525752849724
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620333
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4,
      "acc_stderr": 0.030151134457776285,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.030151134457776285
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3611111111111111,
      "acc_stderr": 0.040166600304512336,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.040166600304512336
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.42196531791907516,
      "acc_stderr": 0.0376574669386515,
      "acc_norm": 0.42196531791907516,
      "acc_norm_stderr": 0.0376574669386515
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.043364327079931785,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.043364327079931785
    },
    "hendrycksTest-computer_security": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3404255319148936,
      "acc_stderr": 0.030976692998534436,
      "acc_norm": 0.3404255319148936,
      "acc_norm_stderr": 0.030976692998534436
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322004,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322004
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2724867724867725,
      "acc_stderr": 0.022930973071633342,
      "acc_norm": 0.2724867724867725,
      "acc_norm_stderr": 0.022930973071633342
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.31746031746031744,
      "acc_stderr": 0.04163453031302859,
      "acc_norm": 0.31746031746031744,
      "acc_norm_stderr": 0.04163453031302859
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3774193548387097,
      "acc_stderr": 0.027575960723278226,
      "acc_norm": 0.3774193548387097,
      "acc_norm_stderr": 0.027575960723278226
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2561576354679803,
      "acc_stderr": 0.0307127300709826,
      "acc_norm": 0.2561576354679803,
      "acc_norm_stderr": 0.0307127300709826
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5212121212121212,
      "acc_stderr": 0.03900828913737302,
      "acc_norm": 0.5212121212121212,
      "acc_norm_stderr": 0.03900828913737302
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.40404040404040403,
      "acc_stderr": 0.034961309720561266,
      "acc_norm": 0.40404040404040403,
      "acc_norm_stderr": 0.034961309720561266
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.5181347150259067,
      "acc_stderr": 0.036060650018329185,
      "acc_norm": 0.5181347150259067,
      "acc_norm_stderr": 0.036060650018329185
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3717948717948718,
      "acc_stderr": 0.024503472557110936,
      "acc_norm": 0.3717948717948718,
      "acc_norm_stderr": 0.024503472557110936
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.40756302521008403,
      "acc_stderr": 0.03191863374478465,
      "acc_norm": 0.40756302521008403,
      "acc_norm_stderr": 0.03191863374478465
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2847682119205298,
      "acc_stderr": 0.03684881521389023,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.03684881521389023
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5100917431192661,
      "acc_stderr": 0.021432956203453316,
      "acc_norm": 0.5100917431192661,
      "acc_norm_stderr": 0.021432956203453316
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.27314814814814814,
      "acc_stderr": 0.030388051301678116,
      "acc_norm": 0.27314814814814814,
      "acc_norm_stderr": 0.030388051301678116
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4068627450980392,
      "acc_stderr": 0.03447891136353382,
      "acc_norm": 0.4068627450980392,
      "acc_norm_stderr": 0.03447891136353382
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.45147679324894513,
      "acc_stderr": 0.0323936001739747,
      "acc_norm": 0.45147679324894513,
      "acc_norm_stderr": 0.0323936001739747
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4663677130044843,
      "acc_stderr": 0.033481800170603065,
      "acc_norm": 0.4663677130044843,
      "acc_norm_stderr": 0.033481800170603065
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4122137404580153,
      "acc_stderr": 0.04317171194870254,
      "acc_norm": 0.4122137404580153,
      "acc_norm_stderr": 0.04317171194870254
    },
    "hendrycksTest-international_law": {
      "acc": 0.5950413223140496,
      "acc_stderr": 0.04481137755942469,
      "acc_norm": 0.5950413223140496,
      "acc_norm_stderr": 0.04481137755942469
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4351851851851852,
      "acc_stderr": 0.04792898170907062,
      "acc_norm": 0.4351851851851852,
      "acc_norm_stderr": 0.04792898170907062
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3803680981595092,
      "acc_stderr": 0.03814269893261836,
      "acc_norm": 0.3803680981595092,
      "acc_norm_stderr": 0.03814269893261836
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.26785714285714285,
      "acc_stderr": 0.04203277291467764,
      "acc_norm": 0.26785714285714285,
      "acc_norm_stderr": 0.04203277291467764
    },
    "hendrycksTest-management": {
      "acc": 0.44660194174757284,
      "acc_stderr": 0.04922424153458935,
      "acc_norm": 0.44660194174757284,
      "acc_norm_stderr": 0.04922424153458935
    },
    "hendrycksTest-marketing": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.031937057262002924,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.031937057262002924
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.44,
      "acc_stderr": 0.049888765156985884,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.049888765156985884
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5070242656449553,
      "acc_stderr": 0.01787819900343221,
      "acc_norm": 0.5070242656449553,
      "acc_norm_stderr": 0.01787819900343221
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4046242774566474,
      "acc_stderr": 0.026424816594009852,
      "acc_norm": 0.4046242774566474,
      "acc_norm_stderr": 0.026424816594009852
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2837988826815642,
      "acc_stderr": 0.015078358970751764,
      "acc_norm": 0.2837988826815642,
      "acc_norm_stderr": 0.015078358970751764
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4411764705882353,
      "acc_stderr": 0.028431095444176643,
      "acc_norm": 0.4411764705882353,
      "acc_norm_stderr": 0.028431095444176643
    },
    "hendrycksTest-philosophy": {
      "acc": 0.4758842443729904,
      "acc_stderr": 0.028365041542564577,
      "acc_norm": 0.4758842443729904,
      "acc_norm_stderr": 0.028365041542564577
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4351851851851852,
      "acc_stderr": 0.0275860062216077,
      "acc_norm": 0.4351851851851852,
      "acc_norm_stderr": 0.0275860062216077
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3262411347517731,
      "acc_stderr": 0.027968453043563168,
      "acc_norm": 0.3262411347517731,
      "acc_norm_stderr": 0.027968453043563168
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3318122555410691,
      "acc_stderr": 0.012026088259897637,
      "acc_norm": 0.3318122555410691,
      "acc_norm_stderr": 0.012026088259897637
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4963235294117647,
      "acc_stderr": 0.030372015885428195,
      "acc_norm": 0.4963235294117647,
      "acc_norm_stderr": 0.030372015885428195
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.39215686274509803,
      "acc_stderr": 0.01975172650876263,
      "acc_norm": 0.39215686274509803,
      "acc_norm_stderr": 0.01975172650876263
    },
    "hendrycksTest-public_relations": {
      "acc": 0.41818181818181815,
      "acc_stderr": 0.0472457740573157,
      "acc_norm": 0.41818181818181815,
      "acc_norm_stderr": 0.0472457740573157
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4775510204081633,
      "acc_stderr": 0.03197694118713672,
      "acc_norm": 0.4775510204081633,
      "acc_norm_stderr": 0.03197694118713672
    },
    "hendrycksTest-sociology": {
      "acc": 0.47761194029850745,
      "acc_stderr": 0.035319879302087305,
      "acc_norm": 0.47761194029850745,
      "acc_norm_stderr": 0.035319879302087305
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.57,
      "acc_norm_stderr": 0.04975698519562428
    },
    "hendrycksTest-virology": {
      "acc": 0.3855421686746988,
      "acc_stderr": 0.037891344246115496,
      "acc_norm": 0.3855421686746988,
      "acc_norm_stderr": 0.037891344246115496
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5730994152046783,
      "acc_stderr": 0.03793620616529917,
      "acc_norm": 0.5730994152046783,
      "acc_norm_stderr": 0.03793620616529917
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "/home/v-liuyifei/llm-awq/llama2-hf/llama-2-7b",
    "model_args": null,
    "num_fewshot": 5,
    "batch_size": 1,
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null,
    "load_state_dict": "Experiments/llama-2-7b-w3-g128/qsd.pt"
  },
  "average_acc": 0.3985349326458845
}