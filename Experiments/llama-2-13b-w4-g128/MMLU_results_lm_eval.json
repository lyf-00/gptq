{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252606,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252606
    },
    "hendrycksTest-anatomy": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.043163785995113245,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.043163785995113245
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5131578947368421,
      "acc_stderr": 0.04067533136309174,
      "acc_norm": 0.5131578947368421,
      "acc_norm_stderr": 0.04067533136309174
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.6113207547169811,
      "acc_stderr": 0.030000485448675986,
      "acc_norm": 0.6113207547169811,
      "acc_norm_stderr": 0.030000485448675986
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.04155319955593147,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.04155319955593147
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5202312138728323,
      "acc_stderr": 0.03809342081273957,
      "acc_norm": 0.5202312138728323,
      "acc_norm_stderr": 0.03809342081273957
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.043364327079931785,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.043364327079931785
    },
    "hendrycksTest-computer_security": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4,
      "acc_stderr": 0.032025630761017346,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.032025630761017346
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2982456140350877,
      "acc_stderr": 0.043036840335373146,
      "acc_norm": 0.2982456140350877,
      "acc_norm_stderr": 0.043036840335373146
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.5172413793103449,
      "acc_stderr": 0.04164188720169375,
      "acc_norm": 0.5172413793103449,
      "acc_norm_stderr": 0.04164188720169375
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30687830687830686,
      "acc_stderr": 0.023752928712112143,
      "acc_norm": 0.30687830687830686,
      "acc_norm_stderr": 0.023752928712112143
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.04134913018303316,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.04134913018303316
    },
    "hendrycksTest-global_facts": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6483870967741936,
      "acc_stderr": 0.027162537826948458,
      "acc_norm": 0.6483870967741936,
      "acc_norm_stderr": 0.027162537826948458
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.4433497536945813,
      "acc_stderr": 0.03495334582162933,
      "acc_norm": 0.4433497536945813,
      "acc_norm_stderr": 0.03495334582162933
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.57,
      "acc_norm_stderr": 0.04975698519562428
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6545454545454545,
      "acc_stderr": 0.03713158067481913,
      "acc_norm": 0.6545454545454545,
      "acc_norm_stderr": 0.03713158067481913
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7121212121212122,
      "acc_stderr": 0.03225883512300992,
      "acc_norm": 0.7121212121212122,
      "acc_norm_stderr": 0.03225883512300992
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7875647668393783,
      "acc_stderr": 0.029519282616817234,
      "acc_norm": 0.7875647668393783,
      "acc_norm_stderr": 0.029519282616817234
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5025641025641026,
      "acc_stderr": 0.025350672979412195,
      "acc_norm": 0.5025641025641026,
      "acc_norm_stderr": 0.025350672979412195
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2851851851851852,
      "acc_stderr": 0.027528599210340496,
      "acc_norm": 0.2851851851851852,
      "acc_norm_stderr": 0.027528599210340496
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5672268907563025,
      "acc_stderr": 0.032183581077426124,
      "acc_norm": 0.5672268907563025,
      "acc_norm_stderr": 0.032183581077426124
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.32450331125827814,
      "acc_stderr": 0.03822746937658753,
      "acc_norm": 0.32450331125827814,
      "acc_norm_stderr": 0.03822746937658753
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7431192660550459,
      "acc_stderr": 0.01873249292834247,
      "acc_norm": 0.7431192660550459,
      "acc_norm_stderr": 0.01873249292834247
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4675925925925926,
      "acc_stderr": 0.034028015813589656,
      "acc_norm": 0.4675925925925926,
      "acc_norm_stderr": 0.034028015813589656
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7450980392156863,
      "acc_stderr": 0.030587591351604246,
      "acc_norm": 0.7450980392156863,
      "acc_norm_stderr": 0.030587591351604246
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7046413502109705,
      "acc_stderr": 0.02969633871342288,
      "acc_norm": 0.7046413502109705,
      "acc_norm_stderr": 0.02969633871342288
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6502242152466368,
      "acc_stderr": 0.03200736719484503,
      "acc_norm": 0.6502242152466368,
      "acc_norm_stderr": 0.03200736719484503
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5954198473282443,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.5954198473282443,
      "acc_norm_stderr": 0.043046937953806645
    },
    "hendrycksTest-international_law": {
      "acc": 0.6859504132231405,
      "acc_stderr": 0.042369647530410184,
      "acc_norm": 0.6859504132231405,
      "acc_norm_stderr": 0.042369647530410184
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6574074074074074,
      "acc_stderr": 0.045879047413018105,
      "acc_norm": 0.6574074074074074,
      "acc_norm_stderr": 0.045879047413018105
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6503067484662577,
      "acc_stderr": 0.03746668325470022,
      "acc_norm": 0.6503067484662577,
      "acc_norm_stderr": 0.03746668325470022
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25,
      "acc_stderr": 0.04109974682633932,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04109974682633932
    },
    "hendrycksTest-management": {
      "acc": 0.7378640776699029,
      "acc_stderr": 0.04354631077260595,
      "acc_norm": 0.7378640776699029,
      "acc_norm_stderr": 0.04354631077260595
    },
    "hendrycksTest-marketing": {
      "acc": 0.7692307692307693,
      "acc_stderr": 0.027601921381417586,
      "acc_norm": 0.7692307692307693,
      "acc_norm_stderr": 0.027601921381417586
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.55,
      "acc_stderr": 0.049999999999999996,
      "acc_norm": 0.55,
      "acc_norm_stderr": 0.049999999999999996
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7381864623243933,
      "acc_stderr": 0.01572083867844527,
      "acc_norm": 0.7381864623243933,
      "acc_norm_stderr": 0.01572083867844527
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.6271676300578035,
      "acc_stderr": 0.026033890613576277,
      "acc_norm": 0.6271676300578035,
      "acc_norm_stderr": 0.026033890613576277
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.3776536312849162,
      "acc_stderr": 0.01621414875213663,
      "acc_norm": 0.3776536312849162,
      "acc_norm_stderr": 0.01621414875213663
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6470588235294118,
      "acc_stderr": 0.027363593284684965,
      "acc_norm": 0.6470588235294118,
      "acc_norm_stderr": 0.027363593284684965
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6559485530546624,
      "acc_stderr": 0.026981478043648043,
      "acc_norm": 0.6559485530546624,
      "acc_norm_stderr": 0.026981478043648043
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6574074074074074,
      "acc_stderr": 0.026406145973625672,
      "acc_norm": 0.6574074074074074,
      "acc_norm_stderr": 0.026406145973625672
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.38652482269503546,
      "acc_stderr": 0.029049190342543454,
      "acc_norm": 0.38652482269503546,
      "acc_norm_stderr": 0.029049190342543454
    },
    "hendrycksTest-professional_law": {
      "acc": 0.4172099087353325,
      "acc_stderr": 0.012593959992906422,
      "acc_norm": 0.4172099087353325,
      "acc_norm_stderr": 0.012593959992906422
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.5330882352941176,
      "acc_stderr": 0.03030625772246831,
      "acc_norm": 0.5330882352941176,
      "acc_norm_stderr": 0.03030625772246831
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.553921568627451,
      "acc_stderr": 0.020109864547181354,
      "acc_norm": 0.553921568627451,
      "acc_norm_stderr": 0.020109864547181354
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6363636363636364,
      "acc_stderr": 0.046075820907199756,
      "acc_norm": 0.6363636363636364,
      "acc_norm_stderr": 0.046075820907199756
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6326530612244898,
      "acc_stderr": 0.03086214492108756,
      "acc_norm": 0.6326530612244898,
      "acc_norm_stderr": 0.03086214492108756
    },
    "hendrycksTest-sociology": {
      "acc": 0.746268656716418,
      "acc_stderr": 0.030769444967296014,
      "acc_norm": 0.746268656716418,
      "acc_norm_stderr": 0.030769444967296014
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.8,
      "acc_stderr": 0.040201512610368466,
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.040201512610368466
    },
    "hendrycksTest-virology": {
      "acc": 0.45180722891566266,
      "acc_stderr": 0.03874371556587953,
      "acc_norm": 0.45180722891566266,
      "acc_norm_stderr": 0.03874371556587953
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7660818713450293,
      "acc_stderr": 0.03246721765117826,
      "acc_norm": 0.7660818713450293,
      "acc_norm_stderr": 0.03246721765117826
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "/home/v-liuyifei/llm-awq/llama2-hf/llama-2-13b",
    "model_args": null,
    "num_fewshot": 5,
    "batch_size": 1,
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null,
    "load_state_dict": "Experiments/llama-2-13b-w4-g128/qsd.pt"
  },
  "average_acc": 0.5471610871729239
}