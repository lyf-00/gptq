{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4666666666666667,
      "acc_stderr": 0.043097329010363554,
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.043097329010363554
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4144736842105263,
      "acc_stderr": 0.04008973785779206,
      "acc_norm": 0.4144736842105263,
      "acc_norm_stderr": 0.04008973785779206
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4641509433962264,
      "acc_stderr": 0.030693675018458003,
      "acc_norm": 0.4641509433962264,
      "acc_norm_stderr": 0.030693675018458003
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04155319955593146,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04155319955593146
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.35,
      "acc_stderr": 0.04793724854411019,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.04793724854411019
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.43352601156069365,
      "acc_stderr": 0.03778621079092055,
      "acc_norm": 0.43352601156069365,
      "acc_norm_stderr": 0.03778621079092055
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.04280105837364395,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "hendrycksTest-computer_security": {
      "acc": 0.56,
      "acc_stderr": 0.0498887651569859,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.0498887651569859
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4297872340425532,
      "acc_stderr": 0.03236214467715563,
      "acc_norm": 0.4297872340425532,
      "acc_norm_stderr": 0.03236214467715563
    },
    "hendrycksTest-econometrics": {
      "acc": 0.32456140350877194,
      "acc_stderr": 0.04404556157374768,
      "acc_norm": 0.32456140350877194,
      "acc_norm_stderr": 0.04404556157374768
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.42758620689655175,
      "acc_stderr": 0.04122737111370333,
      "acc_norm": 0.42758620689655175,
      "acc_norm_stderr": 0.04122737111370333
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2671957671957672,
      "acc_stderr": 0.02278967314577657,
      "acc_norm": 0.2671957671957672,
      "acc_norm_stderr": 0.02278967314577657
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.040061680838488774,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488774
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.4838709677419355,
      "acc_stderr": 0.028429203176724555,
      "acc_norm": 0.4838709677419355,
      "acc_norm_stderr": 0.028429203176724555
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3399014778325123,
      "acc_stderr": 0.0333276906841079,
      "acc_norm": 0.3399014778325123,
      "acc_norm_stderr": 0.0333276906841079
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6242424242424243,
      "acc_stderr": 0.03781887353205982,
      "acc_norm": 0.6242424242424243,
      "acc_norm_stderr": 0.03781887353205982
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4898989898989899,
      "acc_stderr": 0.035616254886737454,
      "acc_norm": 0.4898989898989899,
      "acc_norm_stderr": 0.035616254886737454
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6683937823834197,
      "acc_stderr": 0.03397636541089118,
      "acc_norm": 0.6683937823834197,
      "acc_norm_stderr": 0.03397636541089118
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4128205128205128,
      "acc_stderr": 0.024962683564331806,
      "acc_norm": 0.4128205128205128,
      "acc_norm_stderr": 0.024962683564331806
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.42016806722689076,
      "acc_stderr": 0.03206183783236152,
      "acc_norm": 0.42016806722689076,
      "acc_norm_stderr": 0.03206183783236152
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.271523178807947,
      "acc_stderr": 0.03631329803969653,
      "acc_norm": 0.271523178807947,
      "acc_norm_stderr": 0.03631329803969653
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5981651376146789,
      "acc_stderr": 0.021020106172997013,
      "acc_norm": 0.5981651376146789,
      "acc_norm_stderr": 0.021020106172997013
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2638888888888889,
      "acc_stderr": 0.030058202704309846,
      "acc_norm": 0.2638888888888889,
      "acc_norm_stderr": 0.030058202704309846
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4950980392156863,
      "acc_stderr": 0.03509143375606786,
      "acc_norm": 0.4950980392156863,
      "acc_norm_stderr": 0.03509143375606786
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6455696202531646,
      "acc_stderr": 0.03113730429718581,
      "acc_norm": 0.6455696202531646,
      "acc_norm_stderr": 0.03113730429718581
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5336322869955157,
      "acc_stderr": 0.03348180017060306,
      "acc_norm": 0.5336322869955157,
      "acc_norm_stderr": 0.03348180017060306
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5419847328244275,
      "acc_stderr": 0.04369802690578756,
      "acc_norm": 0.5419847328244275,
      "acc_norm_stderr": 0.04369802690578756
    },
    "hendrycksTest-international_law": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.04345724570292534,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.04345724570292534
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5370370370370371,
      "acc_stderr": 0.04820403072760628,
      "acc_norm": 0.5370370370370371,
      "acc_norm_stderr": 0.04820403072760628
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5214723926380368,
      "acc_stderr": 0.03924746876751129,
      "acc_norm": 0.5214723926380368,
      "acc_norm_stderr": 0.03924746876751129
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.375,
      "acc_stderr": 0.04595091388086298,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.04595091388086298
    },
    "hendrycksTest-management": {
      "acc": 0.5631067961165048,
      "acc_stderr": 0.049111471073657764,
      "acc_norm": 0.5631067961165048,
      "acc_norm_stderr": 0.049111471073657764
    },
    "hendrycksTest-marketing": {
      "acc": 0.7051282051282052,
      "acc_stderr": 0.029872577708891193,
      "acc_norm": 0.7051282051282052,
      "acc_norm_stderr": 0.029872577708891193
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.644955300127714,
      "acc_stderr": 0.017112085772772994,
      "acc_norm": 0.644955300127714,
      "acc_norm_stderr": 0.017112085772772994
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5202312138728323,
      "acc_stderr": 0.026897049996382875,
      "acc_norm": 0.5202312138728323,
      "acc_norm_stderr": 0.026897049996382875
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.46405228758169936,
      "acc_stderr": 0.02855582751652878,
      "acc_norm": 0.46405228758169936,
      "acc_norm_stderr": 0.02855582751652878
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5819935691318328,
      "acc_stderr": 0.028013651891995072,
      "acc_norm": 0.5819935691318328,
      "acc_norm_stderr": 0.028013651891995072
    },
    "hendrycksTest-prehistory": {
      "acc": 0.49691358024691357,
      "acc_stderr": 0.027820214158594384,
      "acc_norm": 0.49691358024691357,
      "acc_norm_stderr": 0.027820214158594384
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3546099290780142,
      "acc_stderr": 0.028538650028878638,
      "acc_norm": 0.3546099290780142,
      "acc_norm_stderr": 0.028538650028878638
    },
    "hendrycksTest-professional_law": {
      "acc": 0.36962190352020863,
      "acc_stderr": 0.01232844577857526,
      "acc_norm": 0.36962190352020863,
      "acc_norm_stderr": 0.01232844577857526
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4889705882352941,
      "acc_stderr": 0.030365446477275668,
      "acc_norm": 0.4889705882352941,
      "acc_norm_stderr": 0.030365446477275668
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.44281045751633985,
      "acc_stderr": 0.02009508315457735,
      "acc_norm": 0.44281045751633985,
      "acc_norm_stderr": 0.02009508315457735
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5181818181818182,
      "acc_stderr": 0.04785964010794915,
      "acc_norm": 0.5181818181818182,
      "acc_norm_stderr": 0.04785964010794915
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4816326530612245,
      "acc_stderr": 0.03198761546763127,
      "acc_norm": 0.4816326530612245,
      "acc_norm_stderr": 0.03198761546763127
    },
    "hendrycksTest-sociology": {
      "acc": 0.6119402985074627,
      "acc_stderr": 0.03445789964362749,
      "acc_norm": 0.6119402985074627,
      "acc_norm_stderr": 0.03445789964362749
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.67,
      "acc_stderr": 0.047258156262526094,
      "acc_norm": 0.67,
      "acc_norm_stderr": 0.047258156262526094
    },
    "hendrycksTest-virology": {
      "acc": 0.40963855421686746,
      "acc_stderr": 0.03828401115079022,
      "acc_norm": 0.40963855421686746,
      "acc_norm_stderr": 0.03828401115079022
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7076023391812866,
      "acc_stderr": 0.03488647713457922,
      "acc_norm": 0.7076023391812866,
      "acc_norm_stderr": 0.03488647713457922
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "/scratch/yifei/llama2-hf/llama-2-7b",
    "model_args": null,
    "num_fewshot": 5,
    "batch_size": 16,
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": null
  },
  "average_acc": 0.4602505381037743
}